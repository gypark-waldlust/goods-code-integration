import csv
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
import numpy as np

def preprocess_text(text):
    # 1. Remove content inside square brackets, parentheses, and curly braces
    text = re.sub(r'\([^)]*\)|\[[^]]*\]|\{[^}]*\}', '', text)
    
    # --- ğŸ’¡ ì¡°ê±´ë¶€ ì˜ì–´ ì œê±° ë¡œì§ ì‹œì‘ ---
    
    # 2a. í…ìŠ¤íŠ¸ì— í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    # 're.search'ëŠ” íŒ¨í„´ì´ ë¬¸ìì—´ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    has_korean = re.search(r'[ê°€-í£]', text)
    
    if has_korean:
        # 2b-1. í•œê¸€ì´ ìˆìœ¼ë©´: í•œê¸€ê³¼ ê³µë°±ë§Œ ë‚¨ê¸°ê³  (ì˜ì–´ë„ ì œê±°)
        # í•œê¸€([^ê°€-í£\s])ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì(ì˜ì–´, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì) ì œê±°
        text = re.sub(r'[^ê°€-í£\s]', '', text)
    else:
        # 2b-2. í•œê¸€ì´ ì—†ìœ¼ë©´: ì˜ì–´, ìˆ«ì, í•œê¸€ì„ ì œì™¸í•œ íŠ¹ìˆ˜ ë¬¸ìë§Œ ì œê±° (ì˜ì–´ ë³´ì¡´)
        # íŠ¹ìˆ˜ ë¬¸ì/ìˆ«ì([^ê°€-í£a-zA-Z\s])ë§Œ ì œê±°
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        # ì°¸ê³ : í˜„ì¬ 1ë‹¨ê³„ì—ì„œ í•œê¸€ì€ ì´ë¯¸ ì œê±°ë˜ì—ˆìœ¼ë¯€ë¡œ, [^a-zA-Z\s]ë§Œìœ¼ë¡œë„ ì¶©ë¶„í•©ë‹ˆë‹¤.
        
    # --- ì¡°ê±´ë¶€ ì˜ì–´ ì œê±° ë¡œì§ ë ---

    # 3. Remove extra whitespace
    text = ' '.join(text.split())
    
    return text

def create_map_from_csv(file_path):
    result_map = {}
    
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        
        # Get the first row as keys
        try:
            keys = next(reader)
        except StopIteration:
            print("Error: File is empty")
            return {}
            
        # Initialize lists for each key
        for key in keys:
            result_map[key] = []
            
        # Iterate through the rest of the rows
        for row in reader:
            for i, value in enumerate(row):
                if i < len(keys):
                    # Store RAW value here to preserve alignment and codes
                    # Preprocessing will be done during clustering
                    result_map[keys[i]].append(value)
                    
    return result_map

from sklearn.metrics.pairwise import cosine_similarity

def classify_data_by_targets(codes, names, targets):
    # Preprocess names for classification
    cleaned_names = [preprocess_text(name) for name in names]
    
    # Combine targets and cleaned names for vectorization to ensure same feature space
    all_texts = targets + cleaned_names
    
    # Vectorize
    # Use char n-grams to capture partial matches (e.g. "ì—ì´ë“œ" inside "ë ˆëª¬ì—ì´ë“œ")
    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))
    try:
        tfidf_matrix = vectorizer.fit_transform(all_texts)
    except ValueError:
        print("Error: No valid data to classify.")
        return {}, {}, None

    # Split matrix back into targets and items
    target_vectors = tfidf_matrix[:len(targets)]
    item_vectors = tfidf_matrix[len(targets):]
    
    # Calculate similarity between items and targets
    # Shape: (n_items, n_targets)
    similarity_matrix = cosine_similarity(item_vectors, target_vectors)
    
    # Classify
    clusters = {}
    cluster_indices = {}
    
    # Initialize clusters for all targets
    for target in targets:
        clusters[target] = []
        cluster_indices[target] = []
    clusters['Unclassified'] = []
    cluster_indices['Unclassified'] = []
    
    threshold = 0.1 # Minimum similarity to be classified
    
    for idx, (code, name, cleaned_name) in enumerate(zip(codes, names, cleaned_names)):
        if not cleaned_name.strip():
            continue
            
        # Find best matching target
        similarities = similarity_matrix[idx]
        best_target_idx = np.argmax(similarities)
        best_score = similarities[best_target_idx]
        
        if best_score >= threshold:
            target = targets[best_target_idx]
            clusters[target].append((code, name, cleaned_name, best_score))
            cluster_indices[target].append(idx)
        else:
            clusters['Unclassified'].append((code, name, cleaned_name, 0.0))
            cluster_indices['Unclassified'].append(idx)
            
    return clusters, cluster_indices

if __name__ == "__main__":
    file_path = 'waldpos_public_base_goods.csv'
    data_map = create_map_from_csv(file_path)
    
    keys = list(data_map.keys())
    print(f"Keys found: {keys}")
    
    if len(keys) >= 2:
        # Assume Col 0 is Code, Col 1 is Name
        col_code = keys[0]
        col_name = keys[1]
        
        codes = data_map[col_code]
        names = data_map[col_name]
        
        # Verify alignment
        if len(codes) != len(names):
            print(f"Warning: Column lengths mismatch! Codes: {len(codes)}, Names: {len(names)}")
            min_len = min(len(codes), len(names))
            codes = codes[:min_len]
            names = names[:min_len]
        
        # Define Targets
        targets = [
            "ì•„ë©”ë¦¬ì¹´ë…¸", "ë¼ë–¼", "í”„ë¼í˜", "ìŠ¤ë¬´ë””", "ì—ì´ë“œ", "í‹°", "ì°¨", 
            "ì¥¬ìŠ¤", "ìš”ê±°íŠ¸", "ë²„ë¸”í‹°", "ë””ì €íŠ¸", "ì¼€ì´í¬", "ë¹µ", "ë² ì´ê¸€", 
            "í•«ë„ê·¸", "ì¿ í‚¤", "ë§ˆì¹´ë¡±", "ì„¸íŠ¸"
        ]
        
        print(f"Classifying {len(codes)} items into {len(targets)} targets...")
        clusters, cluster_indices = classify_data_by_targets(codes, names, targets)
        
        if clusters:
            output_file = 'clustering_results.txt'
            print(f"Writing results to {output_file}...")
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("=== Target-based Classification Results ===\n")
                
                # Sort targets by count
                sorted_targets = sorted(clusters.keys(), key=lambda k: len(clusters[k]), reverse=True)
                
                for target in sorted_targets:
                    items = clusters[target]
                    
                    f.write(f"\n[Target: {target}] (Count: {len(items)})\n")
                    
                    # Print items in cluster
                    # Sort by score desc
                    items.sort(key=lambda x: x[3], reverse=True)
                    
                    for code, name, cleaned, score in items:
                        f.write(f"  - [{code}] {name} -> (Cleaned: {cleaned}) [Sim: {score:.4f}]\n")
            
            print("Done.")
    else:
        print("Error: Need at least 2 columns (Code and Name) to cluster.")


